# 快速体验 OceanBase（单机部署）

本文以三节点集群为例进行安装部署。这里需要注意的是，OB 集群使用到的服务器数量比较多，因此使用命令行进行安装部署时，可能需要在每一台服务器上进行操作。

## 服务器和介质准备 

OceanBase 数据库支持部署在 x86_64 以及 ARM_64 架构的物理服务器和主流的虚拟机，操作系统支持主流的 Linux 发行版本。

### 服务器架构及Linux 操作系统版本要求 

操作系统需要配置网络和 yum/zypper 源：


|            Linux 操作系统             |    版本     |         Architecture         |
|-----------------------------------|-----------|------------------------------|
| Aliyun Linux                      | 7.2 及以上   | x86_64 (包括海光)，ARM_64(仅鲲鹏,飞腾) |
| CentOS / Red Hat Enterprise Linux | 7.2 及以上   | x86_64 (包括海光)，ARM_64(仅鲲鹏,飞腾) |
| SUSE Enterprise Linux             | 12SP3 及以上 | x86_64 (包括海光)，ARM_64(仅鲲鹏,飞腾) |
| Debian / Ubuntu                   | 8.3 及以上   | x86_64 (包括海光)，ARM_64(仅鲲鹏,飞腾) |



### 服务器数量及配置 

最小安装需要准备 4 台服务器：


|                 | 数量  |     功能测试最小配置      |          性能测试最小配置          |
|-----------------|-----|-------------------|----------------------------|
| OCP 管控服务器       | 1 台 | 32C，128G，1.5TB 存储 | 32C，128G，1.5TB SSD 存储，万兆网卡 |
| OceanBase 计算服务器 | 3 台 | 32C，128G，1.2TB 存储 | 32C，256G，2TB SSD 存储，万兆网卡   |



如果需要 OCP 管控服务提供高可用能力，则需要 3 台管控服务器，并提供负载均衡软件或者硬件，如 F5，阿里云 SLB，或者使用 OB 独立版提供的 ob_dns 软负载组件。

### OceanBase 安装包 

软件：Docker CE 17.03 及以上安装包；各组件安装包，OAT 、OCP docker image、OBServer docker image、OBProxy docker image 、OceanBase RPM 包、OBProxy RPM 包

#### 服务器及操作系统参数设置 

### BIOS设置 

进入BIOS设置，修改服务器相关选项的设置：



| **BIOS需要关闭选项:**                           | **BIOS位置**                                                                                       | **备注**                  |
|-------------------------------------------|--------------------------------------------------------------------------------------------------|-------------------------|
| Numa 关闭                                   | UEFI Setup \> system setting \> memory \> socket interleave                                      |                         |
| Cstate 关闭                                 | UEFI Setup \> system settings \> processors \> C-states                                          | 电源最大性能，此参数状态为关闭         |
| Pstate 关闭                                 | UEFI Setup \> system settings \> processors \> cpu p-state control                               | 电源最大性能，此参数状态为关闭         |
| EIST 关闭                                   | 已向联想确认 SR650 无此参数                                                                                |                         |
| Power saving 关闭                           | 已向联想确认 SR650 无此参数                                                                                |                         |
| Turbo Mode 关闭                             | UEFI Setup \> system settings \> operating modes \> Turbo Mode                                   | 电源最大性能，此参数 enable 且无法调整 |
| BMC share link 关闭                         | UEFI Setup \> BMC Settings \> Network Settings \> Network Interface port                         | 默认独占模式                  |
| **BIOS 需要开启项:**                           |                                                                                                  |                         |
| 启动顺序1：HDD \> Network \> USB               | Web 登录带外 \> Server Configuration  \>  Boot Options 中调整，调整后需重启服务器                                 |                         |
| 启动顺序2：HDD 启动项中，第一块盘作为第一启动项                | UEFI 自动识别，不需要                                                                                    |                         |
| Powerloserestorepolicy：always on          | UEFI Setup \> BMC Settings \> Power restore policy                                               |                         |
| 网卡 PXE 启动：开启                              | UEFI Setup \> system settings \> network \> network stack settings \> IPv4 PXE Support           |                         |
| Intel Virtualization Technology：开启        | UEFI Setup \> system settings \> processors \> Intel Virtualization Technology                   |                         |
| SOL console redirection(串口重定向)：开启         | 已向联想确认 SR650 服务器无串口                                                                              |                         |
| Hyper-threading：开启                        | UEFI Setup \> system settings \> processors \> Hyper-threading                                   |                         |
| Hardware Perfetcher：开启                    | UEFI Setup \> system settings \> processors \> Hardware Perfetcher                               |                         |
| VT-d：开启                                   | UEFI Setup \> system settings \> device and I/O ports                                            |                         |
| SR-IOV：开启                                 | UEFI Setup \> system settings \> network \> 10Gb 网卡（X722 网卡无此参数不需调整） \> device configurtion menu |                         |
| CSM boot：Legacy                           | 已向联想确认 SR650 无此参数，此参数为上一代服务器参数，如果设置为 Legacy，那么无法设置 UEFI 启动模式                                     |                         |
| BMC network configuration 开启 DHCP(根据现场情况) | 孔维鑫确认不需设置为 DHCP                                                                                  |                         |
| Energy performance：开启最大performance        | UEFI Setup \> system settings \> operating modes                                                 |                         |



### 磁盘挂载 

* 3 台 OBServer 服务器，挂载盘信息如下：

  

  |    挂载点     |      大小      |       用途       |         文件系统格式         |
  |------------|--------------|----------------|------------------------|
  | /home      | 100-300 GB   | observer 运行日志盘 | 建议 ext4                |
  | /data/log1 | 内存大小的 3-4 倍  | observer 日志盘   | 建议 ext4                |
  | /data/1    | 取决于所需存储的数据大小 | observer 数据盘   | 建议 ext4，大于 16TB 使用 xfs |

  

  挂载磁盘命令如下：

  ```unknow
  # lsblk
  # lvextend -L 100G /dev/vg00/lvhome
  # xfs_growfs /dev/vg00/lvhome
  
  # pvcreate /dev/sdb /dev/sdc
  # vgcreate vg_ob_log /dev/sdb
  # vgcreate vg_ob_data /dev/sdc
  # lvcreate -l +100%free -n lv_ob_log vg_ob_log
  # lvcreate -l +100%free -n lv_ob_data vg_ob_data
  # mkfs.ext4 /dev/vg_ob_log/lv_ob_log
  # mkfs.ext4 /dev/vg_ob_data/lv_ob_data
  # mkdir -p /data/log1
  # mkdir -p /data/1
  # vi/etc/fstab
  /dev/vg_ob_log/lv_ob_log/data/log1        ext4  defaults  0 0
  /dev/vg_ob_data/lv_ob_data/data/1       ext4  defaults  0 0
  # mount -a
  # df --h
  ```

  

* 1 台 OCP 服务器，挂载盘信息如下：

  

  |    挂载点     |      大小      |       用途       |          磁盘格式           |
  |------------|--------------|----------------|-------------------------|
  | /home      | 100 GB       | 各组件运行日志盘       | 建议 ext4                 |
  | /data/log1 | 内存大小的 3-4 倍  | OCP 元数据库 OB日志盘 | 建议 ext4                 |
  | /data/1    | 取决于所需存储的数据大小 | OCP 元数据库 OB日志盘 | 建议 ext4，大于 200GB 使用 xfs |
  | /docker    | 200 GB       | docker 根目录     | 建议 ext4                 |

  

  对于 OCP 服务器，还需配置 docker 目录，命令如下：

  ```unknow
  # pvcreate /dev/sdd
  # vgcreate vg_ob_docker /dev/sdd
  # lvcreate -L 200G -n lv_ob_docker vg_ob_docker
  # mkfs.ext4 /dev/vg_ob_docker/lv_ob_docker
  # vi/etc/fstab
  /dev/vg_ob_docker/lv_ob_docker/docker     ext4    defaults  0 0
  # mount -a
  # df --h
  ```

  
>**说明**
>
>docker 内外目录映射的时候没有做 uid 映射转换，所以默认要求内外目录的 owner 都是 admin 并且 uid 是 500。否则，可能面临目录权限问题。

  若机器上没有 `admin` 用户，请参照如下命令创建。

  ```unknow
  # groupadd -g 500 admin
  # useradd -g 500 -u 500 admin -d /home/admin
  ```


### 网卡设置 

这里以 teaming 网卡配置为例，进行配置网卡，网卡名字设置为 team0，team1，按如下命令进行配置：

1. 创建新主连接 con-name 是 team0，team1：

   ```unknow
   nmcli connection add type team ifname team0 con-name team0 config '{"runner": {"name":"activebackup"}}'
   nmcli connection add type team ifname team1 con-name team1 config '{"runner": {"name":"activebackup"}}'
   ```

   

2. 配置对应的IP、网关，自动连接信息：

   ```unknow
   nmcli connection modify team0 ipv4.addresses 3.27.xx.xx/xx ipv4.gateway 3.27.xx.xxx ipv4.method manual connection.autoconnect yes
   nmcli connection modify team1 ipv4.addresses 192.168.x.xx/xx ipv4.method manual connection.autoconnect yes
   ```

   

3. 创建从接口：

   ```unknow
   nmcli connection add type team-slave con-name team0-port1 ifname ens1f0 master team0
   nmcli connection add type team-slave con-name team0-port2 ifname ens4f0 master team0
   nmcli connection add type team-slave con-name team1-port1 ifname ens1f1 master team1
   nmcli connection add type team-slave con-name team1-port2 ifname ens4f1 master team1
   ```

   

4. 开启主接口和从接口：

   ```unknow
   nmcli connection reload
   nmcli connection up team0
   nmcli connection up team1
   ```

   

5. 查看网卡状态：

   ```unknow
   teamdctl team0 state
   ```

   




### 配置时钟源 

OceanBase 数据库是一个分布式数据库，所以集群多个节点，以及 OCP 节点的时钟必须配置时钟同步服务 NTP 或者 chrony ，保证所有节点时钟偏差在 100ms 以内。

1. 安装 ntp 相关软件。

   ```unknow
   yum -y install ntp ntpdate
   ```

   

2. 配置 ntp.conf

   ```unknow
   # vim /etc/ntp.conf 
   restrict default ignore 
   restrict 127.0.0.1 
   restrict 192.168.0.0 mask 255.255.0.0
   
   driftfile /var/lib/ntp/drift 
   pidfile /var/run/ntpd.pid 
   #logfile /var/log/ntp.log 
   
   # local clock 
   server 127.127.1.0 
   fudge 127.127.1.0 stratum 10 
   
   server 192.168.1.100    iburst minpoll 4 maxpoll 6
   ```

   

   restrict 用于指定 IP 的相关 时间同步命令 权限 。

   ```unknow
   restrict [IP] mask [netmask_IP] [parameter]
   ```

   

   parameter 包括 ignore、nomodify、noquery 、notrap 和 notrust 等。ignore 默认指拒绝所有类型的 NTP 同步。

   server 用于指定上层 NTP 源服务器。

   ```unknow
   server [IP or hostname] [prefer]
   ```

   

   如果没有上层 NTP 源服务器，可以设置为 127.127.1.0，即本机作为 NTP 源服务器。
   

3. 重启 NTP 同步服务

   ```unknow
   # systemctl restart ntpd 
   # systemctl status ntpd 
   ● ntpd.service - Network Time Service Loaded: 
      loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: enabled) 
      Active: active (running) since Sat 2020-04-18 14:43:31 CST; 10s ago 
     Process: 95939 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS) 
    Main PID: 95944 (ntpd) 
      Memory: 544.0K 
      CGroup: /system.slice/ntpd.service 
              └─95944 /usr/sbin/ntpd -u ntp:ntp -g 
   ......
   ```

   

4. 检查 NTP 同步状态

   1. 运行 ntpstat 命令。如果 ntp 服务刚启动，运行命令是没有结果的，需要等待一会。

      ```unknow
      # ntpstat 
      unsynchronised
      time server re-starting 
      polling server every 64 s
      ```

      
   
   2. 此后再跑

      ```unknow
      # ntpstat 
      synchronised to NTP server (192.168.1.100) at stratum 3 
      time correct to within 8 ms 
      polling server every 64 s
      ```

      
   
   3. 查看 ntpq

      ```unknow
      [root@hostname /]# ntpq -p|grep -E "\*|\=|remote"
         remote         refid           st t when poll reach    delay     offset     jitter
      ==========================================================================
      *xxxxxxxxxxx       xxxxx           2 u   16  64  377       1.333     0.046   0.033
      ```

      

      remote : 表示使用的 ntp 服务器。 `*` 表示目前选择的 NTP 服务器。

      refid ：远程 NTP 服务器使用的更高一级 NTP 服务器。
      st：远程 NTP 服务器的 stratum（级别）。
      when ：最后一次同步到现在的时间（单位默认为秒）。
      poll ：同步的频率 ，单位 秒。
      delay ：从本机到远程 NTP 服务器的往返时间（单位毫秒）。
      offset ：本机与远程 NTP 服务器的时间偏移量（单位毫秒）。

      jitter ：本机与远程 NTP 服务器的时间偏移量平均偏差（单位毫秒）。
      
   

   

5. 最终同步延时检查以下面为准：

   ```unknow
   # clockdiff 192.168.xx.xxx 
   .................................................. 
   host=192.168.xx.xxx rtt=1(0)ms/1ms delta=0ms/0ms Sat Apr 18 14:41:40 2020
   ```

   




### 软件配置 

我们需要在 OCP 服务器上安装 Docker 且为 Docker CE 17.03 及以上版本。然后才可以进行部署 OAT。

部署 OAT 具体步骤如下：上传 OAT 安装包到 OCP 服务器，挂载 OAT 目录（这里需要 2 个目录，一个用于存放 OAT 系统日志及 Datax 与 OBLA 生成的文件，一个用于存放 OAT 的数据库文件），将 OAT 安装包装载为镜像，获取 OAT 镜像的标签，启动 OAT。相应的命令行参考如下：

```unknow
[root@hostname /]# scp oat.tar <oat_server>:/oat.tar
[root@oat_server /]# mkdir -p /oat/data_dir /oat/db_dir
[root@oat_server /]# docker load -i oat.tar
[root@oat_server /]# oat_image=`docker images | grep oat | awk '{printf $1":"$2"\n"}'`
[root@oat_server /]# docker run -d -e DEPLOY_MODE=generic -v /oat/data_dir:/data -v /oat/db_dir:/var/lib/mysql -p 7000:7000 --restart on-failure:5 $oat_image
```



到这里，部署 OAT 就完成了，我们可以验证下是否成功。

在浏览器中输入 `http://<OAT 服务器地址>:7000`，在弹出的界面使用 admin 账号登录系统，这里默认密码为 root，登录成功，说明部署成功了。

在安装 OAT 后，OAT 会自动在本地 `/oat/data_dir/images` 目录下创建以组件名称命名的目录，您需要将安装包上传到这些目录中。例如，您需要将 OCP 的安装包 `ocp.tar` 上传到 `/oat/data_dir/images/ocp` 目录中。相应的命令行参考如下：

```unknow
[root@hostname /]# scp ./ocp.tar <oat_server>:/oat/data_dir/images/ocp/ocp.tar
[root@hostname /]# scp ./obproxy.tar <oat_server>:/oat/data_dir/images/obproxy/obproxy.tar
[root@hostname /]# scp ./observer.tar <oat_server>:/oat/data_dir/images/observer/observer.tar
[root@hostname /]# scp ./dns.tar <oat_server>:/oat/data_dir/images/dns/dns.tar
```



部署 
-----------------------

### 部署 OCP 

软硬件都准备好以后，可以进行正式安装部署了。切换到刚才登录的 OAT 界面，在这里进行相关的操作。

1. 部署 OCP 之前，需要先添加 OCP 服务器。

   1. 在左侧导航栏中，单击 **产品部署** \> **服务器管理** 。

      
   
   2. 在页面右上角，单击 **添加服务器** 。

      
   
   3. 在弹出的 **添加服务器** 对话框中，输入服务器信息。

      
   
   4. 单击 **确认** 。

      添加成功后，可以在服务器列表中看到新添加的服务器。
      
   

   

2. 初始化 OCP 服务器。

   1. 在左侧导航栏上，单击 **服务器管理** \> **服务器管理** 。

      
   
   2. 找到刚才添加的服务器，在对应的 **操作** 列中，单击 **初始化** 。

      
   
   3. 在弹出的对话框中，选择服务器用途为 **OCP** 并为服务器设置 Admin 用户的密码，单击 **初始化** 。

      
   
   4. 在弹出的提示框中，单击 **已完成** 。

      单击 **已完成** 后，系统开始对服务器进行初始化操作，包括创建装机用户、设置操作系统内核、安装依赖包等。同时，系统还会创建一个任务，您可以在 **任务管理** 页面查看当前任务的执行情况。
      
   

   

3. 新增 OCP 配置文件。

   1. 在左侧导航栏上，单击 **产品部署** \> **配置文件** 。

      
   
   2. 在页面右上角，单击 **新增配置文件** \> **OCP** ，进入 **新增OCP配置文件** 页面。

      
   
   3. 在 **基本信息** 向导页面，填写配置文件基本信息，并单击 **下一步** 。

      
   
   4. 进行负载均衡配置，并单击 **下一步** 。

      默认 **负载均衡模式** 为 **none** ，无需修改。
      
   
   5. 进行 Meta OB 容器配置，并单击 **下一步** 。

      
   
   6. 进行 OBProxy 容器配置，并单击 **下一步** 。

      
   
   7. 进行 OCP Server 容器配置。

      
   
   8. 单击 **确定** 。

      系统会生成一个任务流，开始部署 OCP，您可以在 **任务管理** 中查看该任务的执行情况。
      
   

   

4. 部署 OCP

   1. 在左侧导航栏上，单击 **产品部署** \> **产品管理** 。

      
   
   2. 在 OCP 图标左下角，单击 **立即部署** 。

      
   
   3. 选择之前配置的 OCP 配置文件，单击 **确认** 。

      单击 **确认** 后，系统会提示正在部署中。同时，还会创建一个部署任务，您可以单击产品区域中的 **查看详情** 链接，查看当前部署任务的执行情况。
      
   

   




OCP 部署完成后，我们来验证下是否部署成功。在浏览器输入 `http://<` *`OCP 服务器的 IP 地址`* `>:8080`，在弹出的登录页面，使用默认账号 `admin` 账号登录，默认密码为 `root` ，进行验证。

### 部署 OceanBase 集群 

这里注意，要用刚才的 OCP 界面进行操作了。

#### 添加主机 

进入 OCP 前端页面，默认的用户名/密码：`admin/root`。在 OCP 左侧导航栏，点击 **主机** ，点击右上角蓝色的 **添加主机** 按钮，在弹出的对话框中根据提示补充相关信息，添加新主机。具体如下所示。![添加主机](../images/p431562.png)点击 **添加主机** 后，在弹出来的页面，补充 主机 IP，机型，机房等相关信息后，点击确定![添加主机详情](../images/p431564.png)

#### 创建 OB 集群 

进入 OCP 页面，默认的用户名/密码：`admin/root`。在 OCP 首页，点击右上角蓝色的 **创建集群** 按钮，在弹出的对话框中根据提示补充相关信息，创建新集群。具体如下所示。此时需要注意，已获取并上传需要的OceanBase RPM 版本包。![新建集群详情](../images/p431568.png)如果此时机房中，机器不够，则会出现上图中的红色提示，此时根据需要，可以选择删除该 zone 或者添加对应的主机即可。

到这里，就完成了 OB 集群的安装部署，您可以在已部署成功的集群进行相关的数据库操作，比如创建租户、创建数据库、创建用户，创建表等操作。

安装部署集群的更多信息可以参考官网的 《 **部署 OceanBase 数据库** 》

连接数据库（MySQL 模式） 
------------------------------------

部署完成后，数据库中默认创建 sys 租户。此后根据需要新建租户后，通过租户连接数据库即可使用数据库了。

这里新建一个 MySQL 模式的 emp 租户，并新建对应的数据库 employees 以及对应的用户 employee，然后进行简单的连接示例。

### 新建 emp 租户 

这里推荐使用 OCP 创建需要的租户。OCP 集成 多条黑屏操作命令，使用更为简单方便。

在 OCP 左侧导航栏，点击 **租户** ，点击右上角蓝色的 **新建租户** 按钮，在弹出的对话框中根据提示补充相关信息，创建所需租户。这里以 emp 租户为示例进行创建。该租户使用全能型副本，有且只有一个副本，资源为最小规格的 s1，即 `1.5C6G`，且只有一个 unit，这里创建的是 MySQL 模式的租户，![新建租户](../images/p431238.png)点击 **提交** 后，会创建出 emp 租户。如下图所示，

![新建租户结果图](../images/p431250.png)新建 emp 租户后，系统会默认创建 `OceanBase`、`information_sachema`、`mysql`、`test` 4个默认数据库。

### 新建 employees 数据库 

在租户列表中，点击 **emp** 租户名，会出现 emp 租户总览页面，点击左侧导航栏的 **数据库管理** ，点击右上角的 **新建数据库** ，创建 employees 数据库。如下图所示。![新建数据库](../images/p431525.png)

点击 **新建数据库** 后，在弹出的对话框中，填上数据库名 employees，其他没有特殊要求可以不用修改，默认配置即可，点击提交，创建 employees 数据库。具体如下图。

![新建数据库详情](../images/p431528.png)

### 新建 employee 用户 

在租户列表中，点击 **emp** 租户名，会出现 emp 租户总览页面，点击左侧导航栏的 **用户管理** ，点击右上角的 **新建** 用户 ，创建 employee 用户。如下图所示。![新建用户](../images/p431529.png)点击 **新建用户** 后，在弹出的对话框补充用户名 **employee** 以及根据需要勾选所需的权限，点击提交即可创建employee 用户。![新建用户详情](../images/p431541.png)

### 连接 employees 数据库 

在建好的 emp 租户总览页中，点击 OBProxy/连接串右侧的查看，可以看到当前数据库的连接串，使用该连接串即可连接到 employees 数据库。

连接 OceanBase 数据库的方式，这里命令行推荐使用 OceanBase 客户端（OBClient） 即 OceanBase 数据库专用的命令行客户端工具，白屏推荐 OceanBase 开发者中心（OceanBase Developer Center，ODC）。

* 使用 OBClient 连接数据库

  从官网下载并安装 OBClient 后，打开命令行工具，这里以 MINGW64 为例，这里使用 间接连接的方式，首先要跳转到 OBproxy 所在主机![跳转跳板机](../images/p431348.png)然后连接 employees 数据库。![连接数据库](../images/p431547.png)
  

* 使用 ODC 连接数据库。

  具体方式请参见 开发者入门 中的连接数据库。
  




连接数据库（Oracle 模式） 
-------------------------------------

Oracle 模式的连接数据库的过程跟 MySQL 模式基本相同，有以下几点不同需要注意：

* 创建的 Oracle 租户跟 MySQL 租户在模式选择上有区别，需要选择 Oracle 模式。

  

* Oracle 模式无需创建数据库，直接创建用户既可使用。

  

* Oracle 模式在创建用户的时候，一般创建对应的角色，用于后续权限管理。

  




除此之外，Oracle 模式连接数据库跟 MySQL 模式是完全一致的。

此时，就可以进行数据库使用和管理了。

更多详情，请参见 **《OceanBase 云平台》** 和 **《OceanBase 数据库》** 。
